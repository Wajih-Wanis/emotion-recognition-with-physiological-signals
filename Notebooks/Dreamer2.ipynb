{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NWMy7g_wNHl"
      },
      "source": [
        "Ce bloc de code importe plusieurs bibliothèques de Python nécessaires pour l'analyse de données et la création de modèles de classification. Voici ce que font les bibliothèques importées :\n",
        "\n",
        "sklearn.model_selection : cette bibliothèque fournit des fonctions pour la sélection de modèles, la validation croisée et la sélection de paramètres.\n",
        "\n",
        "sklearn.linear_model : cette bibliothèque fournit des classes pour la régression linéaire.\n",
        "\n",
        "sklearn.neural_network : cette bibliothèque fournit des classes pour les réseaux de neurones.\n",
        "\n",
        "sklearn.neighbors : cette bibliothèque fournit des classes pour la classification kNN.\n",
        "\n",
        "sklearn.svm : cette bibliothèque fournit des classes pour les machines à vecteurs de support.\n",
        "\n",
        "sklearn.gaussian_process : cette bibliothèque fournit des classes pour les processus gaussiens.\n",
        "\n",
        "sklearn.tree : cette bibliothèque fournit des classes pour les arbres de décision.\n",
        "\n",
        "sklearn.ensemble : cette bibliothèque fournit des classes pour les méthodes d'ensemble, comme Random Forest et AdaBoost.\n",
        "\n",
        "sklearn.naive_bayes : cette bibliothèque fournit des classes pour la classification bayésienne naïve.\n",
        "\n",
        "sklearn.discriminant_analysis : cette bibliothèque fournit des classes pour l'analyse discriminante linéaire et quadratique.\n",
        "\n",
        "sklearn.preprocessing : cette bibliothèque fournit des classes pour la prétraitement des données, telles que la normalisation et la mise à l'échelle.\n",
        "\n",
        "sklearn.pipeline : cette bibliothèque fournit des classes pour la construction de pipelines de traitement de données.\n",
        "\n",
        "scipy.signal : cette bibliothèque fournit des fonctions pour le traitement du signal.\n",
        "\n",
        "tensorflow : cette bibliothèque fournit des classes pour la création de modèles de réseau de neurones.\n",
        "\n",
        "tensorflow.keras : cette bibliothèque fournit des fonctions pour la création de modèles de réseau de neurones avec Keras.\n",
        "\n",
        "matplotlib.pyplot : cette bibliothèque fournit des fonctions pour la création de graphiques et de visualisations.\n",
        "\n",
        "scipy.io : cette bibliothèque fournit des fonctions pour lire et écrire des fichiers MATLAB.\n",
        "\n",
        "neurokit2 : cette bibliothèque fournit des fonctions pour l'analyse de données psychophysiologiques.\n",
        "\n",
        "seaborn : cette bibliothèque fournit des fonctions pour la visualisation de données statistiques.\n",
        "\n",
        "pandas : cette bibliothèque fournit des classes pour la manipulation de données en tableau.\n",
        "\n",
        "numpy : cette bibliothèque fournit des fonctions pour le calcul numérique.\n",
        "\n",
        "time : cette bibliothèque fournit des fonctions pour la mesure du temps.\n",
        "\n",
        "mne : cette bibliothèque fournit des fonctions pour l'analyse de données d'électroencéphalographie (EEG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPh_Z_nSwxpI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import svm\n",
        "from scipy import signal\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "\n",
        "!pip install neurokit2\n",
        "import neurokit2 as nk\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "!pip install mne\n",
        "import mne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lguzS9RwR4z"
      },
      "source": [
        "Ce bloc de code permet de monter votre lecteur Google Drive dans Google Colab. Cela vous permet de charger des fichiers à partir de votre lecteur Google Drive directement dans votre environnement Colab.\n",
        "\n",
        "La fonction drive.mount() prend en paramètre le chemin d'accès au dossier de montage. Une fois que la commande est exécutée, vous serez invité à autoriser l'accès à votre lecteur Google Drive. Lorsque vous aurez donné votre autorisation, le lecteur sera monté et vous pourrez accéder à son contenu à partir de votre environnement Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKswZsRnxPcF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqhgwySnwUvI"
      },
      "source": [
        "Ce bloc de code charge un fichier de données DREAMERedit.mat à partir du chemin d'accès spécifié dans la variable path en utilisant la fonction sio.loadmat() de la bibliothèque scipy.io.\n",
        "\n",
        "La fonction sio.loadmat() charge les données à partir d'un fichier MATLAB .mat et les stocke sous forme de dictionnaire Python. La variable raw contient les données chargées à partir du fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKlqeXuVxXAJ"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/My Drive/DREAMER.mat\"\n",
        "raw = sio.loadmat(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o48qdcPweNp"
      },
      "source": [
        "La section ci-dessous concerne les signaux EEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-IUs4CwXO4"
      },
      "source": [
        "# EEG SIGNALS "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoMHTmKaC0E6"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "electrode = 0\n",
        "video = 0\n",
        "participant = 0\n",
        "\n",
        "stim = raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, electrode]\n",
        "basl = raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"baseline\"][0, 0][video, 0][:, electrode]\n",
        "\n",
        "\n",
        "fig = make_subplots(rows=2, cols=1, shared_xaxes=True)\n",
        "\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(len(stim))), y=stim, name=\"Stimulus Signal\"),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=list(range(len(basl))), y=basl, name=\"Baseline Signal\"),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=\"Time (samples)\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Signal Amplitude\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Signal Amplitude\", row=2, col=1)\n",
        "\n",
        "fig.update_layout(title_text=\"EEG Signals\")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twh6DXD7woiN"
      },
      "source": [
        "Cette fonction effectue un prétraitement sur un signal EEG brut. Le signal est découpé en 12 segments de 640 points de données chacun, puis chaque segment est filtré avec un filtre passe-bas de fréquence de coupure de 40 Hz. Ensuite, la fonction calcule la densité spectrale de puissance (PSD) de chaque segment en utilisant la méthode Welch. Les PSDs sont ensuite utilisées pour calculer la moyenne de la puissance dans les bandes de fréquences theta (4-7.5 Hz), alpha (7.5-13 Hz) et beta (13-20 Hz) pour chaque segment. Ces moyennes sont stockées dans une liste de caractéristiques. La fonction renvoie cette liste de caractéristiques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ8_ETGixjS9"
      },
      "outputs": [],
      "source": [
        "def preprocess_EEG(raw, feature):\n",
        "\n",
        "  frame = [[0 for x in range(640)] for y in range(12)] \n",
        "\n",
        "  for i in range(0,7680):\n",
        "    j = i//640\n",
        "    k = i%640\n",
        "    frame[j][k] = raw[i]\n",
        "  \n",
        "  samplingRate = 128 \n",
        "  nyquest = samplingRate/2.0\n",
        "  n=213\n",
        "\n",
        "  f1 = 40/nyquest\n",
        "  \n",
        "\n",
        "  fnew = [[0 for x in range(640)] for y in range(18)]\n",
        "  pnew = [[0 for x in range(640)] for y in range(18)]\n",
        "  thetamean =[]\n",
        "  alphamean =[]\n",
        "  betamean  =[]\n",
        "\n",
        "  overall_filter = signal.firwin(n, f1, pass_zero=True, window=\"hamming\") \n",
        "\n",
        "  for i in range(0,12):\n",
        "\n",
        "    filt_overall = signal.filtfilt(overall_filter, 1, frame[i], method ='pad')\n",
        "    f,p = signal.welch(filt_overall,fs =128, nperseg=256)\n",
        "    \n",
        "    fnew[i] = f\n",
        "    pnew[i] = p\n",
        "    \n",
        "    plt.semilogy(fnew[i], pnew[i])\n",
        "    plt.xlabel('frequency [Hz]')\n",
        "    plt.ylabel('PSD')\n",
        "\n",
        "    c =[]\n",
        "    for k in range(0,129):\n",
        "      x=[pnew[i][k],fnew[i][k]]\n",
        "      c.append(x)\n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] >4.0 and c[a][1] <=7.5 ): #theta\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    thetamean.append(mean)\n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] >7.5 and c[a][1]<=13):   #alpha\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    alphamean.append(mean)\n",
        "\n",
        "    count = 0.0; sum = 0.0; mean = 0.0\n",
        "    for a in range(0,129):\n",
        "      if(c[a][1] >13 and c[a][1]<=20):    #beta\n",
        "        count +=1\n",
        "        sum += c[a][0] \n",
        "    mean = sum/count\n",
        "    betamean.append(mean)\n",
        "\n",
        "\n",
        "    fe=[]\n",
        "    fe.append(thetamean[i])\n",
        "    fe.append(alphamean[i])\n",
        "    fe.append(betamean[i])\n",
        "      \n",
        "    feature.append(fe)\n",
        "\n",
        "  return feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEzeGTMl8hvc"
      },
      "source": [
        "Ce code définit une fonction plot_spectral_density qui prend trois paramètres en entrée - electrode, video et participant. Il calcule la densité spectrale des signaux EEG du jeu de données DREAMER pour une électrode, une vidéo et un participant spécifiques. Il applique un filtre FIR fenêtré de type Hamming aux signaux EEG, calcule l'estimation de la densité spectrale de puissance de Welch, et trace les résultats sur une échelle logarithmique. Le graphique comprend deux lignes représentant la densité spectrale des stimuli et de la ligne de base, respectivement. L'axe des x représente la fréquence en Hertz et l'axe des y représente la densité spectrale de puissance en décibels. Le titre du graphique inclut les informations sur l'électrode et le participant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCCpcN78w8gB"
      },
      "outputs": [],
      "source": [
        "def plot_spectral_density(electrode, video, participant):\n",
        "    stim = raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, electrode]\n",
        "    basl = raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"baseline\"][0, 0][video, 0][:, electrode]\n",
        "    frame_stim = [[0 for x in range(640)] for y in range(12)] \n",
        "    frame_basl = [[0 for x in range(640)] for y in range(12)]\n",
        "    for i in range(0,7680):\n",
        "        j = i//640\n",
        "        k = i%640\n",
        "        frame_stim[j][k] = stim[i]\n",
        "    for i in range(0,7680):\n",
        "        j = i//640\n",
        "        k = i%640\n",
        "        frame_basl[j][k] = basl[i]  \n",
        "    samplingRate = 128 \n",
        "    nyquist = samplingRate/2.0\n",
        "    n = 213\n",
        "\n",
        "    f1 = 40/nyquist\n",
        "\n",
        "    fnew_stim = []\n",
        "    pnew_stim = []\n",
        "    fnew_basl = []\n",
        "    pnew_basl = []\n",
        "\n",
        "    overall_filter = signal.firwin(n, f1, pass_zero=True, window=\"hamming\") \n",
        "\n",
        "    for i in range(0, 12):\n",
        "\n",
        "        filt_overall_stim = signal.filtfilt(overall_filter, 1, frame_stim[i], method='pad')\n",
        "        f_stim, p_stim = signal.welch(filt_overall_stim, fs=128, nperseg=256)\n",
        "\n",
        "        fnew_stim.append(f_stim)\n",
        "        pnew_stim.append(p_stim)\n",
        "        \n",
        "    for i in range(0, 12):\n",
        "\n",
        "        filt_overall_basl = signal.filtfilt(overall_filter, 1, frame_basl[i], method='pad')\n",
        "        f_basl, p_basl = signal.welch(filt_overall_basl, fs=128, nperseg=256)\n",
        "\n",
        "        fnew_basl.append(f_basl)\n",
        "        pnew_basl.append(p_basl)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.grid(True)\n",
        "    plt.semilogy(fnew_stim[electrode], pnew_stim[electrode], label='Stimuli')\n",
        "    plt.semilogy(fnew_basl[electrode], pnew_basl[electrode], label='Baseline')\n",
        "    plt.xlabel('Frequency [Hz]')\n",
        "    plt.ylabel('PSD [dB]')\n",
        "    plt.legend()\n",
        "    plt.title('Electrode: {}, Participant: {}, Video {}'.format(electrode, participant,video))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_7a--z-Ttf2"
      },
      "outputs": [],
      "source": [
        "def plot_spectral_density_alpha_beta_theta(electrode, video, participant):\n",
        "    stim = raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, electrode]\n",
        "    basl = raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"baseline\"][0, 0][video, 0][:, electrode]\n",
        "    frame_stim = [[0 for x in range(640)] for y in range(12)] \n",
        "    frame_basl = [[0 for x in range(640)] for y in range(12)]\n",
        "    for i in range(0,7680):\n",
        "        j = i//640\n",
        "        k = i%640\n",
        "        frame_stim[j][k] = stim[i]\n",
        "    for i in range(0,7680):\n",
        "        j = i//640\n",
        "        k = i%640\n",
        "        frame_basl[j][k] = basl[i]  \n",
        "    samplingRate = 128 \n",
        "    nyquist = samplingRate/2.0\n",
        "    n = 213\n",
        "\n",
        "    f1 = 40/nyquist\n",
        "\n",
        "    fnew_stim = []\n",
        "    pnew_stim = []\n",
        "    fnew_basl = []\n",
        "    pnew_basl = []\n",
        "\n",
        "    overall_filter = signal.firwin(n, f1, pass_zero=True, window=\"hamming\") \n",
        "\n",
        "    for i in range(0, 12):\n",
        "\n",
        "        filt_overall_stim = signal.filtfilt(overall_filter, 1, frame_stim[i], method='pad')\n",
        "        f_stim, p_stim = signal.welch(filt_overall_stim, fs=128, nperseg=256)\n",
        "\n",
        "        fnew_stim.append(f_stim)\n",
        "        pnew_stim.append(p_stim)\n",
        "        \n",
        "    for i in range(0, 12):\n",
        "\n",
        "        filt_overall_basl = signal.filtfilt(overall_filter, 1, frame_basl[i], method='pad')\n",
        "        f_basl, p_basl = signal.welch(filt_overall_basl, fs=128, nperseg=256)\n",
        "\n",
        "        fnew_basl.append(f_basl)\n",
        "        pnew_basl.append(p_basl)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.grid(True)\n",
        "    alpha_band = np.where((fnew_stim[electrode] >= 8) & (fnew_stim[electrode] <= 12))\n",
        "    beta_band = np.where((fnew_stim[electrode] >= 12) & (fnew_stim[electrode] <= 30))\n",
        "    theta_band = np.where((fnew_stim[electrode] >= 4) & (fnew_stim[electrode] <= 8))\n",
        "    \n",
        "    plt.semilogy(fnew_stim[electrode][alpha_band], pnew_stim[electrode][alpha_band], label='Alpha')\n",
        "    plt.semilogy(fnew_stim[electrode][beta_band], pnew_stim[electrode][beta_band], label='Beta')\n",
        "    plt.semilogy(fnew_stim[electrode][theta_band], pnew_stim[electrode][alpha_band], label='Theta')\n",
        "    plt.xlabel('Frequency [Hz]')\n",
        "    plt.ylabel('PSD [dB]')\n",
        "    plt.legend()\n",
        "    plt.title('Electrode: {}, Participant: {}, Video {}'.format(electrode, participant,video))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKFQ6ECnxUZi"
      },
      "outputs": [],
      "source": [
        "plot_spectral_density(2,2,1)\n",
        "plot_spectral_density_alpha_beta_theta(2,2,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57XVIwndBQuR"
      },
      "source": [
        "Cette fonction a pour but d'extraire des caractéristiques à partir de signaux EEG brut. Les données brutes sont stockées dans une variable \"raw\" et les électrodes à partir desquelles les caractéristiques seront extraites sont stockées dans une variable \"electrode\".\n",
        "\n",
        "La fonction commence par initialiser une matrice EEG_tmp de dimensions (23,18,36). Cette matrice sera utilisée pour stocker les caractéristiques extraites de chaque participant pour chaque vidéo.\n",
        "\n",
        "La fonction parcourt ensuite les participants et les vidéos et extrait les signaux de base et de stimulation pour chaque participant et vidéo. Les signaux de base et de stimulation sont ensuite envoyés à la fonction preprocess_EEG qui calcule la densité spectrale de puissance (PSD) pour chaque électrode. Les PSD de la stimulation sont divisées par les PSD de base pour obtenir la densité relative. Les valeurs de densité relative sont stockées dans la matrice EEG_tmp.\n",
        "\n",
        "La fonction se termine en créant un tableau de données pandas à partir de la matrice EEG_tmp, en redimensionnant le tableau de données pour avoir 36 colonnes et en normalisant les données à l'aide de la classe StandardScaler de scikit-learn. Enfin, la fonction renvoie le tableau de données pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aBtRZ1YyZ0e"
      },
      "outputs": [],
      "source": [
        "def feat_extract_EEG(raw, electrode):\n",
        "    EEG_tmp = np.zeros((23, 18, 36))\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "\n",
        "              B, S = [], []\n",
        "                \n",
        "              basl = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"baseline\"][0, 0][video, 0][:, electrode])\n",
        "              stim = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, electrode])\n",
        "                \n",
        "              B = preprocess_EEG(basl, B) \n",
        "              S = preprocess_EEG(stim, S) \n",
        "\n",
        "              A = np.divide(S, B)   \n",
        "              column = 0\n",
        "\n",
        "              for k in range(0,3):\n",
        "                for l in range(0,12):\n",
        "                  EEG_tmp[participant, video,column] = A[l][k]\n",
        "                  column+=1\n",
        "                                \n",
        "\n",
        "    col = []\n",
        "\n",
        "    for i in range(0, 12):\n",
        "      col.append(\"psdtheta_\"+str(i + 1)+\"_un\")\n",
        "    for i in range(0, 12):\n",
        "      col.append(\"psdalpha_\"+str(i + 1)+\"_un\")\n",
        "    for i in range(0, 12):\n",
        "     col.append(\"psdbeta_\"+str(i + 1)+\"_un\")\n",
        "\n",
        "    EEG_tmp = EEG_tmp.reshape(-1,EEG_tmp.shape[2])\n",
        "    data_EEG = pd.DataFrame(EEG_tmp, columns=col)\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    \n",
        "    for i in range(len(col)):\n",
        "        data_EEG[col[i][:-3]] = data_EEG[[col[i]]]\n",
        "        \n",
        "    data_EEG.drop(col, axis=1, inplace=True)\n",
        "    return data_EEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "den4CK6IChUx"
      },
      "source": [
        "Cette fonction prend en entrée un objet 'raw' et crée un tableau 'a' de taille (23, 18, 9). Ensuite, elle parcourt les 23 participants et les 18 vidéos pour lesquels les données de valence, arousal et dominance doivent être prédites. Dans chaque itération de la boucle, la fonction remplit les premières colonnes de 'a' avec l'âge, le genre, le numéro de participant et le numéro de vidéo correspondants. La fonction remplit également les noms des vidéos et les émotions cibles pour chaque vidéo, ainsi que les scores de valence, d'excitation et de domination correspondants extraits de l'objet 'raw'.\n",
        "\n",
        "Enfin, la fonction convertit 'a' en un objet DataFrame de pandas en réorganisant les données en 18*23 lignes et 9 colonnes correspondant à l'âge, le genre, le numéro de participant, le numéro de vidéo, le nom de la vidéo, l'émotion cible, la valence, l'excitation et la domination. La fonction retourne le DataFrame 'b'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKp6hdrZy22F"
      },
      "outputs": [],
      "source": [
        "def participant_affective(raw):\n",
        "    a = np.zeros((23, 18, 9), dtype=object)\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "            a[participant, video, 0] = (raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                        [0, participant][\"Age\"][0][0][0])\n",
        "            \n",
        "            a[participant, video, 1] = (raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                        [0, participant][\"Gender\"][0][0][0])\n",
        "            \n",
        "            a[participant, video, 2] = int(participant+1)\n",
        "            \n",
        "            a[participant, video, 3] = int(video+1)\n",
        "            \n",
        "            a[participant, video, 4] = [\"Searching for Bobby Fischer\",\n",
        "                                        \"D.O.A.\", \"The Hangover\", \"The Ring\",\n",
        "                                        \"300\", \"National Lampoon\\'s VanWilder\",\n",
        "                                        \"Wall-E\", \"Crash\", \"My Girl\",\n",
        "                                        \"The Fly\", \"Pride and Prejudice\",\n",
        "                                        \"Modern Times\", \"Remember the Titans\",\n",
        "                                        \"Gentlemans Agreement\", \"Psycho\",\n",
        "                                        \"The Bourne Identitiy\",\n",
        "                                        \"The Shawshank Redemption\",\n",
        "                                        \"The Departed\"][video]\n",
        "            \n",
        "            a[participant, video, 5] = [\"calmness\", \"surprise\", \"amusement\",\n",
        "                                        \"fear\", \"excitement\", \"disgust\",\n",
        "                                        \"happiness\", \"anger\", \"sadness\",\n",
        "                                        \"disgust\", \"calmness\", \"amusement\",\n",
        "                                        \"happiness\", \"anger\", \"fear\",\n",
        "                                        \"excitement\", \"sadness\",\n",
        "                                        \"surprise\"][video]\n",
        "            a[participant, video, 6] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreValence\"]\n",
        "                                           [0, 0][video, 0])\n",
        "            a[participant, video, 7] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreArousal\"]\n",
        "                                           [0, 0][video, 0])\n",
        "            a[participant, video, 8] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreDominance\"]\n",
        "                                           [0, 0][video, 0])\n",
        "    b = pd.DataFrame(a.reshape((23*18, a.shape[2])),\n",
        "                     columns=[\"age\", \"gender\", \"participant\",\n",
        "                              \"video\", \"video_name\", \"target_emotion\",\n",
        "                              \"valence\", \"arousal\", \"dominance\"])\n",
        "    return b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-iABL-GCmct"
      },
      "source": [
        "Cette cellule doit être exécuté ligne par ligne pour sauvegarder chaque fois les caractéristiques(features) de l'électrode dans une variable séparée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM1V5A02zLqY"
      },
      "outputs": [],
      "source": [
        "#RUN Each one by one till cell where is it savd as csv\n",
        "\n",
        "def_EEG1 = feat_extract_EEG(raw, 0)\n",
        "#def_EEG2 = feat_extract_EEG(raw, 1)\n",
        "#def_EEG3 = feat_extract_EEG(raw, 2)\n",
        "#def_EEG4 = feat_extract_EEG(raw, 3)\n",
        "#def_EEG5 = feat_extract_EEG(raw, 4)\n",
        "#def_EEG6 = feat_extract_EEG(raw, 5)\n",
        "#def_EEG7 = feat_extract_EEG(raw, 6)\n",
        "#def_EEG8 = feat_extract_EEG(raw, 7)\n",
        "#def_EEG9 = feat_extract_EEG(raw, 8)\n",
        "#def_EEG10 = feat_extract_EEG(raw, 9)\n",
        "#def_EEG11 = feat_extract_EEG(raw, 10)\n",
        "#def_EEG12 = feat_extract_EEG(raw, 11)\n",
        "#def_EEG13 = feat_extract_EEG(raw, 12)\n",
        "#def_EEG14 = feat_extract_EEG(raw, 13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1A6v6k9Fr0i"
      },
      "source": [
        "Ce code concatène deux dataframes df_features et df_participant_affective en utilisant la fonction pd.concat(), pour créer le dataframe final df. Les colonnes valence, arousal et dominance de df_participant_affective sont converties en entiers en utilisant la méthode astype(int). Le dataframe final est stocké dans la variable df.\n",
        "\n",
        "Le deuxième dataframe df2 est également créé en concaténant df_features et df_participant_affective avec la fonction pd.concat()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeZj2uDZzdIY"
      },
      "outputs": [],
      "source": [
        "#Choisir l'électrode voulu\n",
        "df_features = def_EEG1\n",
        "\n",
        "df_participant_affective = participant_affective(raw)\n",
        "\n",
        "df_participant_affective[\"valence\"] = (df_participant_affective\n",
        "                                       [\"valence\"].astype(int))\n",
        "df_participant_affective[\"arousal\"] = (df_participant_affective\n",
        "                                       [\"arousal\"].astype(int))\n",
        "df_participant_affective[\"dominance\"] = (df_participant_affective\n",
        "                                         [\"dominance\"].astype(int))\n",
        "\n",
        "df = pd.concat([df_features, df_participant_affective], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx8cOM9HH1AM"
      },
      "source": [
        "Ce code crée un nouveau DataFrame appelé data2 qui est une copie de df contenant uniquement les émotions cibles suivantes : 'anger', 'fear', 'calmness', 'surprise', 'excitement', 'amusement', 'happiness', 'sadness' et 'disgust'. Ensuite, il crée trois nouvelles colonnes dans data2 : 'class', 'valencehigh' et 'arousalhigh'. La colonne 'class' est créée en utilisant un dictionnaire qui associe chaque émotion à une classe numérique allant de 0 à 3. Les colonnes 'valencehigh' et 'arousalhigh' sont créées en utilisant des dictionnaires qui associent les valeurs de valence et d'excitation à des valeurs binaires de 0 ou 1. Enfin, le DataFrame data2 est enregistré en tant que fichier CSV sous le nom 'DE3_NS.csv'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cva-DVctzfra"
      },
      "outputs": [],
      "source": [
        "data2 = df.loc[(df['target_emotion'] == 'anger') |\n",
        "                (df['target_emotion'] == 'fear') |\n",
        "                (df['target_emotion'] == 'calmness') |\n",
        "                (df['target_emotion'] == 'surprise') |\n",
        "                (df['target_emotion'] == 'excitement') |\n",
        "                (df['target_emotion'] == 'amusement') |\n",
        "                (df['target_emotion'] == 'happiness') |\n",
        "                (df['target_emotion'] == 'sadness') |\n",
        "                (df['target_emotion'] == 'disgust')].copy()\n",
        "\n",
        "d={'surprise': 0, 'excitement': 0, 'amusement': 0, 'happiness': 0, 'fear': 1, 'anger': 1, 'sadness':2, 'disgust':2 ,'calmness': 3}\n",
        "data2['class'] = data2.target_emotion.map(d)\n",
        "\n",
        "e={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "data2['valencehigh'] = data2.valence.map(e)\n",
        "\n",
        "f={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "data2['arousalhigh'] = data2.arousal.map(e)\n",
        "\n",
        "data2.to_csv('DE3_NS.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G4IjrT6IDIP"
      },
      "source": [
        "Ce code correspond à la sélection de données EEG brutes (non traitées) pour une paire participant-vidéo-électrode donnée à partir d'un ensemble de données stockées dans un dictionnaire multidimensionnel. Les données sont ensuite prétraitées (filtrage, normalisation) en utilisant une fonction nommée preprocess_EEG(). Les données de base (basl) et de stimulation (stim) sont stockées dans deux listes B et S. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAsGpM5_orxK"
      },
      "outputs": [],
      "source": [
        "participant =0\n",
        "video = 0\n",
        "electrode = 0\n",
        "B, S = [], []\n",
        "                \n",
        "basl = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"baseline\"][0, 0][video, 0][:, electrode])\n",
        "stim = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, electrode])\n",
        "                \n",
        "B = preprocess_EEG(basl, B) \n",
        "S = preprocess_EEG(stim, S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-kEMyA_JG1W"
      },
      "source": [
        "Cette fonction prend des données EEG brutes (non traitées) en entrée et extrait des caractéristiques (features) pertinentes de ces données. Elle organise ces caractéristiques dans un tableau et normalise les valeurs de ces caractéristiques pour qu'elles soient toutes comprises entre 0 et 1. La fonction renvoie ce tableau contenant les caractéristiques extraites normalisées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rBhdNx47rF7N"
      },
      "outputs": [],
      "source": [
        "def feat_extract_EEG_All(raw):\n",
        "    \n",
        "    EEG_tmp = np.zeros((23, 18, 107520))\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "          column=0\n",
        "          for electrode in range(0,14):\n",
        "            stim = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"EEG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, electrode])\n",
        "            for i in range(0,7680):\n",
        "              EEG_tmp[participant,video,column]= stim[i]\n",
        "              column+=1\n",
        "                                \n",
        "\n",
        "    col = []\n",
        "\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode1_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode2_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode3_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "     col.append(\"Electrode4_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode5_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode6_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode7_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode8_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "     col.append(\"Electrode9_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode10_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode11_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode12_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "      col.append(\"Electrode13_\"+str(i + 1))\n",
        "    for i in range(0, 7680):\n",
        "     col.append(\"Electrode14_\"+str(i + 1))\n",
        "\n",
        "\n",
        "    EEG_tmp = EEG_tmp.reshape(-1,EEG_tmp.shape[2])\n",
        "    data_EEG = pd.DataFrame(EEG_tmp, columns=col)\n",
        "    print(data_EEG)\n",
        "    from sklearn import preprocessing\n",
        "\n",
        "    x = data_EEG.values\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "    data_EEG = pd.DataFrame(x_scaled)\n",
        "    data_EEG.columns = col\n",
        "    print(data_EEG)\n",
        "    return data_EEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW-IMYciJUto"
      },
      "source": [
        "Ce code commence par extraire les caractéristiques des signaux EEG bruts en utilisant la fonction \"feat_extract_EEG_All\". Ensuite, il crée des dataframes à partir des caractéristiques et des scores d'affect de chaque participant. Il convertit ensuite les scores de valence, d'excitation et de domination en entiers et concatène les deux dataframes.\n",
        "\n",
        "Le code crée ensuite un nouveau dataframe \"data1\" en filtrant les émotions cibles à partir d'une colonne nommée \"target_emotion\". Il attribue un numéro de classe à chaque émotion cible et crée deux colonnes binaires \"valencehigh\" et \"arousalhigh\" à partir des scores de valence et d'excitation.\n",
        "\n",
        "Enfin, il exporte le dataframe résultant dans un fichier CSV nommé \"normalized.csv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gVs3Z0Fv00Rl"
      },
      "outputs": [],
      "source": [
        "def_all = feat_extract_EEG_All(raw)\n",
        "df_features = def_all\n",
        "df_participant_affective = participant_affective(raw)\n",
        "\n",
        "df_participant_affective[\"valence\"] = (df_participant_affective\n",
        "                                       [\"valence\"].astype(int))\n",
        "df_participant_affective[\"arousal\"] = (df_participant_affective\n",
        "                                       [\"arousal\"].astype(int))\n",
        "df_participant_affective[\"dominance\"] = (df_participant_affective\n",
        "                                         [\"dominance\"].astype(int))\n",
        "dfn = pd.concat([df_features, df_participant_affective], axis=1)\n",
        "data1 = dfn.loc[(df['target_emotion'] == 'anger') |\n",
        "                (df['target_emotion'] == 'fear') |\n",
        "                (df['target_emotion'] == 'calmness') |\n",
        "                (df['target_emotion'] == 'surprise') |\n",
        "                (df['target_emotion'] == 'excitement') |\n",
        "                (df['target_emotion'] == 'amusement') |\n",
        "                (df['target_emotion'] == 'happiness') |\n",
        "                (df['target_emotion'] == 'sadness') |\n",
        "                (df['target_emotion'] == 'disgust')].copy()\n",
        "\n",
        "d={'surprise': 0, 'excitement': 0, 'amusement': 0, 'happiness': 0, 'fear': 1, 'anger': 1, 'sadness':2, 'disgust':2 ,'calmness': 3}\n",
        "data1['class'] = data1.target_emotion.map(d)\n",
        "\n",
        "e={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "data1['valencehigh'] = data1.valence.map(e)\n",
        "\n",
        "f={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "data1['arousalhigh'] = data1.arousal.map(e)\n",
        "\n",
        "data1.to_csv('normalized.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6gmL5BRq4RO"
      },
      "outputs": [],
      "source": [
        "data1 = pd.read_csv(\"normalized.csv\")  \n",
        "data_1 = data1.head()\n",
        "data_1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkC1rKf-wIG-"
      },
      "source": [
        "Dans ce code, on commence par extraire deux colonnes du DataFrame data2 qui sont nommées psdalpha_12 et psdalpha_3. Les valeurs de ces deux colonnes sont ensuite converties en tableaux numpy de type float. Ensuite, ces deux tableaux sont concaténés en un seul tableau numpy df3.\n",
        "\n",
        "Ensuite, on réinitialise l'index de data2, puis on extrait toutes les colonnes à partir de la 2ème colonne jusqu'à la 36ème colonne à l'aide de la méthode iloc. Les colonnes extraites sont stockées dans le DataFrame dfz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9UUjpRmrLbw"
      },
      "outputs": [],
      "source": [
        "\n",
        "df1=data2.reset_index()['psdalpha_12']\n",
        "df2=data2.reset_index()['psdalpha_3']\n",
        "df1 = np.array(df1, dtype = float)\n",
        "df2 = np.array(df2, dtype = float)\n",
        "df3 = np.concatenate((df1, df2), axis=0)\n",
        "data2=data2.reset_index()\n",
        "dfz=data2.iloc[:,1:37]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt7qj6rg7oiK"
      },
      "outputs": [],
      "source": [
        "dfz\n",
        "dfy=pd.DataFrame(data={ 'alpha12':df1,'alpha3':df2})\n",
        "dfz = np.array(dfz, dtype = float)\n",
        "dfz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU6QHp5arNEt"
      },
      "outputs": [],
      "source": [
        "target=data2['valencehigh']\n",
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNHd-GtewT7D"
      },
      "source": [
        "La première ligne importe la fonction train_test_split du module model_selection de la bibliothèque sklearn (Scikit-Learn), qui permet de diviser les données en ensemble d'entraînement et ensemble de test pour l'évaluation d'un modèle de machine learning.\n",
        "\n",
        "Les trois lignes suivantes créent des tableaux numpy contenant les données d'entrée (features) de l'ensemble d'entraînement et de test (df1, df2) ainsi que la variable cible (target). La méthode dtype = float est utilisée pour s'assurer que les données sont stockées sous forme de nombres flottants.\n",
        "\n",
        "Enfin, la dernière ligne applique la fonction train_test_split pour diviser les données en deux ensembles, un pour l'entraînement (x_Train et y_Train) et l'autre pour le test (x_Test et y_Test), avec une proportion de 35% pour l'ensemble de test et une graine aléatoire de 6 pour la reproductibilité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3z1YQQorOvK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df1 = np.array(df1, dtype = float)\n",
        "df2 = np.array(df2, dtype = float)\n",
        "target = np.array(target, dtype = float)\n",
        "\n",
        "\n",
        "x_Train, x_Test, y_Train, y_Test = train_test_split(dfz, target, test_size = 0.35, random_state = 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI8Ty4YGd3P3"
      },
      "outputs": [],
      "source": [
        "y_Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqo8GMsbwf7Q"
      },
      "source": [
        "Ces trois lignes de code préparent les données pour l'entrainement d'un modèle de réseau de neurones en redimensionnant les données d'entraînement et de test pour qu'elles aient la forme (n_samples, n_channels, n_features). Ici, il y a 1 canal car il n'y a qu'un seul type de données (PSD) utilisé, et 36 caractéristiques par canal (caractéristiques extraites du PSD). La fonction reshape de numpy est utilisée pour redimensionner les tableaux en conséquence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1EovMiBrQLD"
      },
      "outputs": [],
      "source": [
        "x_Train = x_Train.reshape(x_Train.shape[0],1,36)\n",
        "x_Test = x_Test.reshape(x_Test.shape[0],1,36)\n",
        "x_Test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnvhyzN0wx0h"
      },
      "source": [
        "Ce code crée un modèle de réseau de neurones pour la classification binaire à partir des données d'entraînement et de test préparées précédemment. Le modèle est créé en utilisant les couches d'entrée, de LSTM et de sortie de Keras, et utilise une fonction d'activation relu pour la couche LSTM et une fonction d'activation sigmoid pour la couche de sortie. L'optimiseur Adamax est utilisé pour minimiser la perte binaire_crossentropy et maximiser la précision du modèle. Le modèle est entraîné sur 1000 epochs avec une taille de lot de 20 et un poids de classe spécifique est donné pour la classification des classes minoritaires. Les résultats de la formation sont stockés dans l'objet d'historique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcxdpRx-rRhq"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.layers as KL\n",
        "inputs = KL.Input(shape=(1,36))\n",
        "x = KL.LSTM(units = 8, activation = 'relu')(inputs)\n",
        "outputs = KL.Dense(units=1, activation= 'sigmoid')(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs, outputs)\n",
        "\n",
        "opt = tf.keras.optimizers.Adamax()\n",
        "model.compile( optimizer=opt , loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "history = model.fit(x_Train, y_Train, batch_size=20, epochs=1000, class_weight={0:1, 1:2},validation_data =(x_Test,y_Test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QJvqG_YaoKq"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x_Test)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux4NRFvBcYUW"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8cSn9qZby2T"
      },
      "outputs": [],
      "source": [
        " plt.plot(history.history['loss'])\n",
        " plt.title('Loss for LSTM-binary model')\n",
        " plt.xlabel('epochs')\n",
        " plt.ylabel('loss')\n",
        " plt.show()\n",
        " plt.plot(history.history['accuracy'])\n",
        " plt.title('Accuracy for LSTM-binary model')\n",
        " plt.xlabel('epochs')\n",
        " plt.ylabel('accuracy')\n",
        " plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wu2gSuTyBMc"
      },
      "source": [
        "Ce code construit un modèle de réseau de neurones à longue mémoire récurrent (LSTM) pour prédire la valence émotionnelle. Les données d'entraînement et de test sont préparées en transformant les tableaux 2D en tableaux 3D avec une dimension supplémentaire pour le temps. Le modèle est construit en utilisant la classe Sequential de Keras, qui permet de définir une pile linéaire de couches de neurones. Le modèle contient deux couches LSTM avec des unités de 128 et 64, respectivement. Des couches Dropout sont également ajoutées pour éviter le surapprentissage. Enfin, deux couches de neurones Denses avec des fonctions d'activation ReLU et Sigmoid sont ajoutées pour produire les prédictions. Le modèle est compilé avec l'optimiseur Adam, la fonction de perte 'sparse_categorical_crossentropy' et la métrique 'accuracy'. Le modèle est ensuite entraîné avec les données d'entraînement et les prédictions sont générées à partir des données de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxxXygTgrUbR"
      },
      "outputs": [],
      "source": [
        "x_Train = x_Train.reshape(x_Train.shape[0],1,36)\n",
        "x_Test = x_Test.reshape(x_Test.shape[0],1,36)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128,activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64,activation ='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32,activation ='relu'))\n",
        "model.add(Dense(2,activation ='sigmoid'))\n",
        "y_pred = model.predict(x_Test)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt,metrics=['accuracy'])\n",
        "history = model.fit(x_Train,y_Train,batch_size=16,epochs=200, validation_data =(x_Test,y_Test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLOGSshLrVxr"
      },
      "outputs": [],
      "source": [
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iGW_jr5rXWN"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7eRme54lbon"
      },
      "outputs": [],
      "source": [
        " plt.plot(history.history['loss'])\n",
        " plt.title(\"Loss for LSTM model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Loss\")\n",
        " plt.show()\n",
        " plt.plot(history.history['accuracy'])\n",
        " plt.title(\"Accuracy for LSTM model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        " plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z2rZDD-yMZr"
      },
      "source": [
        "Le deuxième modèle est un modèle CNN (Convolutional Neural Network). Le modèle a deux couches Dense avec une fonction d'activation ReLU, une couche Flatten qui convertit les données en un vecteur, et une couche Dense avec une fonction d'activation softmax qui produit la sortie en 10 catégories. Le modèle utilise également une fonction de perte \"sparse_categorical_crossentropy\" et l'optimiseur Adam pour la classification en deux catégories. Le modèle est entraîné sur les mêmes données d'apprentissage que le premier modèle, mais pendant 200 époques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCJ8hx3crY1-"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(Dense(128,activation =tf.nn.relu))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation =tf.nn.relu))\n",
        "model.add(Dense(9,activation =tf.nn.softmax))\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt ,metrics=['accuracy'])\n",
        "history_CNN = model.fit(x_Train,y_Train,epochs=200, validation_data =(x_Test,y_Test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8LUx1piraMv"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnn-B7zKrbcE"
      },
      "outputs": [],
      "source": [
        " plt.plot(history_CNN.history['loss'])\n",
        " plt.title(\"Loss for CNN model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Loss\")\n",
        " plt.show()\n",
        " plt.plot(history_CNN.history['accuracy'])\n",
        " plt.title(\"Accuracy for CNN model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        " plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIcwuKnIzpiH"
      },
      "source": [
        "Dans ce code, les données sont préparées pour la prédiction de l'AROUSAL. Les valeurs de psdalpha_1 et psdbeta_1 sont extraites et divisées entre elles, puis les valeurs de psdalpha_12 et psdbeta_12 sont également extraites et divisées. Les deux nouveaux ensembles de données sont ensuite concaténés pour former df8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAnDaFc-rchN"
      },
      "outputs": [],
      "source": [
        "#DATA PREP FOR AROUSAL\n",
        "df4=data2.reset_index()['psdalpha_1']\n",
        "df5=data2.reset_index()['psdbeta_1']\n",
        "\n",
        "df45 = df5/df4\n",
        "\n",
        "df6=data2.reset_index()['psdalpha_12']\n",
        "df7=data2.reset_index()['psdbeta_12']\n",
        "\n",
        "df67  = df6/df7\n",
        "\n",
        "df8 = df45.append(df67)\n",
        "df8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiKVvX1Cre5R"
      },
      "outputs": [],
      "source": [
        "target2 =data2['arousalhigh']\n",
        "target2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KckPJU0jrhxU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "target2 = np.array(target2, dtype = float)\n",
        "x_Train2, x_Test2, y_Train2, y_Test2 = train_test_split(dfz, target2, test_size = 0.35, random_state = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXtT0LFOrjFI"
      },
      "outputs": [],
      "source": [
        "x_Train2 = x_Train2.reshape(x_Train2.shape[0],1,36)\n",
        "x_Test2 = x_Test2.reshape(x_Test2.shape[0],1,36)\n",
        "x_Test2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzCrcfIjz3_U"
      },
      "source": [
        "Ce code crée un modèle de réseaux de neurones LSTM pour la classification de l'arousal. Il utilise la fonction d'activation 'relu' pour les couches LSTM et une fonction d'activation 'softmax' pour la couche de sortie Dense. Le modèle est compilé avec l'optimiseur Adam et une fonction de perte de 'sparse_categorical_crossentropy'. Il est ensuite entraîné sur les données d'entraînement et évalué sur les données de validation pour 1000 époques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI8cLPpMrkYC"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128,activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128,activation ='relu'))\n",
        "model.add(Dense(32,activation ='relu'))\n",
        "model.add(Dense(10,activation ='softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt,metrics=['accuracy'])\n",
        "history = model.fit(x_Train2,y_Train2,epochs=1000, validation_data =(x_Test2,y_Test2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwzTV-7Vrl1i"
      },
      "outputs": [],
      "source": [
        " plt.plot(history.history['loss'])\n",
        " plt.title(\"Loss for LSTM model - Arousal\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Loss\")\n",
        " plt.show()\n",
        " plt.plot(history.history['accuracy'])\n",
        " plt.title(\"Accuracy for LSTM model - Arousal\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        " plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yGxyBJ5z-5X"
      },
      "source": [
        "#ECG SIGNALS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngY-4LZ3CSFb"
      },
      "source": [
        "La fonction \"participant_affective(raw)\" prend en entrée un objet \"raw\". Elle crée une matrice \"a\" de dimensions (23,18,9) remplit de zéros et de type objet. Cette matrice sera remplie de données issues de l'objet \"raw\" pour chaque participant et chaque vidéo. Elle crée ensuite un DataFrame \"b\" de dimensions (23*18, 9) avec les données de la matrice \"a\" et des noms de colonnes spécifiques. Enfin, elle renvoie le DataFrame \"b\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkbsVDdFzWy5"
      },
      "outputs": [],
      "source": [
        "def participant_affective(raw):\n",
        "    a = np.zeros((23, 18, 9), dtype=object)\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "            a[participant, video, 0] = (raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                        [0, participant][\"Age\"][0][0][0])\n",
        "            \n",
        "            a[participant, video, 1] = (raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                        [0, participant][\"Gender\"][0][0][0])\n",
        "            \n",
        "            a[participant, video, 2] = int(participant+1)\n",
        "            \n",
        "            a[participant, video, 3] = int(video+1)\n",
        "            \n",
        "            a[participant, video, 4] = [\"Searching for Bobby Fischer\",\n",
        "                                        \"D.O.A.\", \"The Hangover\", \"The Ring\",\n",
        "                                        \"300\", \"National Lampoon\\'s VanWilder\",\n",
        "                                        \"Wall-E\", \"Crash\", \"My Girl\",\n",
        "                                        \"The Fly\", \"Pride and Prejudice\",\n",
        "                                        \"Modern Times\", \"Remember the Titans\",\n",
        "                                        \"Gentlemans Agreement\", \"Psycho\",\n",
        "                                        \"The Bourne Identitiy\",\n",
        "                                        \"The Shawshank Redemption\",\n",
        "                                        \"The Departed\"][video]\n",
        "            \n",
        "            a[participant, video, 5] = [\"calmness\", \"surprise\", \"amusement\",\n",
        "                                        \"fear\", \"excitement\", \"disgust\",\n",
        "                                        \"happiness\", \"anger\", \"sadness\",\n",
        "                                        \"disgust\", \"calmness\", \"amusement\",\n",
        "                                        \"happiness\", \"anger\", \"fear\",\n",
        "                                        \"excitement\", \"sadness\",\n",
        "                                        \"surprise\"][video]\n",
        "            a[participant, video, 6] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreValence\"]\n",
        "                                           [0, 0][video, 0])\n",
        "            a[participant, video, 7] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreArousal\"]\n",
        "                                           [0, 0][video, 0])\n",
        "            a[participant, video, 8] = int(raw[\"DREAMER\"][0, 0][\"Data\"]\n",
        "                                           [0, participant][\"ScoreDominance\"]\n",
        "                                           [0, 0][video, 0])\n",
        "    b = pd.DataFrame(a.reshape((23*18, a.shape[2])),\n",
        "                     columns=[\"age\", \"gender\", \"participant\",\n",
        "                              \"video\", \"video_name\", \"target_emotion\",\n",
        "                              \"valence\", \"arousal\", \"dominance\"])\n",
        "    return b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV8D_nvWCTlT"
      },
      "source": [
        "Cette fonction extrait les données d'ECG (électrocardiogramme) pour les stimuli et les périodes de base des 23 participants de l'étude DREAMER. Les signaux ECG pour chaque participant et vidéo sont stockés dans des matrices ECG_tmp_stim et ECG_tmp_basl. Les signaux ECG pour chaque participant et vidéo sont stockés dans des matrices ECG_tmp_stim et ECG_tmp_basl. Ensuite, ces données sont normalisées à l'aide de la méthode MinMaxScaler du module preprocessing de sklearn et stockées dans les dataframes data_ECG_stim et data_ECG_basl respectivement. Ces dataframes sont ensuite renvoyés par la fonction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW8OEACx0FxD"
      },
      "outputs": [],
      "source": [
        "def feat_extract_ECG_scaled(raw):\n",
        "    \n",
        "    ECG_tmp_stim = np.zeros((23, 18, 31232))\n",
        "    ECG_tmp_basl = np.zeros((23, 18, 31232))\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "          column=0\n",
        "          for signal in range(0,2):\n",
        "            stim = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"ECG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, signal])\n",
        "            basl = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"ECG\"][0, 0][\"baseline\"][0, 0][video, 0][:, signal])\n",
        "            for i in range(0,15616):\n",
        "              ECG_tmp_stim[participant,video,column]= stim[i]\n",
        "              ECG_tmp_basl[participant,video,column]= basl[i]\n",
        "              column+=1\n",
        "                                \n",
        "\n",
        "    col_stim = []\n",
        "    col_basl = []\n",
        "\n",
        "    for i in range(0, 15616):\n",
        "      col_stim.append(\"RA\"+str(i + 1))\n",
        "    for i in range(0, 15616):\n",
        "      col_stim.append(\"LA\"+str(i + 1))\n",
        "    for i in range(0, 15616):\n",
        "      col_basl.append(\"RA\"+str(i + 1))\n",
        "    for i in range(0, 15616):\n",
        "      col_basl.append(\"LA\"+str(i + 1))\n",
        "\n",
        "\n",
        "\n",
        "    ECG_tmp_stim = ECG_tmp_stim.reshape(-1,ECG_tmp_stim.shape[2])\n",
        "    data_ECG_stim = pd.DataFrame(ECG_tmp_stim, columns=col_stim)\n",
        "\n",
        "    ECG_tmp_basl = ECG_tmp_basl.reshape(-1,ECG_tmp_basl.shape[2])\n",
        "    data_ECG_basl = pd.DataFrame(ECG_tmp_basl, columns=col_basl)\n",
        "\n",
        "    from sklearn import preprocessing\n",
        "\n",
        "    x_stim = data_ECG_stim.values\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled_stim = min_max_scaler.fit_transform(x_stim)\n",
        "    data_ECG_stim = pd.DataFrame(x_scaled_stim)\n",
        "    data_ECG_stim.columns = col_stim\n",
        "\n",
        "    x_basl = data_ECG_basl.values\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled_basl = min_max_scaler.fit_transform(x_basl)\n",
        "    data_ECG_basl = pd.DataFrame(x_scaled_basl)\n",
        "    data_ECG_basl.columns = col_basl\n",
        "    return data_ECG_stim,data_ECG_basl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXuyds42clGt"
      },
      "outputs": [],
      "source": [
        "ecg_stim_data,ecg_basl_data = feat_extract_ECG_scaled(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlRPjednqAKf"
      },
      "outputs": [],
      "source": [
        "ecg_stim_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw2pY78FCv2r"
      },
      "source": [
        "Cette fonction extrait les données d'ECG (électrocardiogramme) pour les stimuli et les périodes de base des 23 participants de l'étude DREAMER. Les signaux ECG pour chaque participant et vidéo sont stockés dans des matrices ECG_tmp_stim et ECG_tmp_basl. Les signaux ECG pour chaque participant et vidéo sont stockés dans des matrices ECG_tmp_stim et ECG_tmp_basl. Ces dataframes sont ensuite renvoyés par la fonction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBpKdH_-thVT"
      },
      "outputs": [],
      "source": [
        "def feat_extract_ECG(raw):\n",
        "    \n",
        "    ECG_tmp_stim = np.zeros((23, 18, 31232))\n",
        "    ECG_tmp_basl = np.zeros((23, 18, 31232))\n",
        "    for participant in range(0, 23):\n",
        "        for video in range(0, 18):\n",
        "          column=0\n",
        "          for signal in range(0,2):\n",
        "            stim = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"ECG\"][0, 0][\"stimuli\"][0, 0][video, 0][:, signal])\n",
        "            basl = (raw[\"DREAMER\"][0, 0][\"Data\"][0, participant][\"ECG\"][0, 0][\"baseline\"][0, 0][video, 0][:, signal])\n",
        "            for i in range(0,15616):\n",
        "              ECG_tmp_stim[participant,video,column]= stim[i]\n",
        "              ECG_tmp_basl[participant,video,column]= basl[i]\n",
        "              column+=1\n",
        "                                \n",
        "\n",
        "    col_stim = []\n",
        "    col_basl = []\n",
        "\n",
        "    for i in range(0, 15616):\n",
        "      col_stim.append(\"RA\"+str(i + 1))\n",
        "    for i in range(0, 15616):\n",
        "      col_stim.append(\"LA\"+str(i + 1))\n",
        "    for i in range(0, 15616):\n",
        "      col_basl.append(\"RA\"+str(i + 1))\n",
        "    for i in range(0, 15616):\n",
        "      col_basl.append(\"LA\"+str(i + 1))\n",
        "\n",
        "\n",
        "\n",
        "    ECG_tmp_stim = ECG_tmp_stim.reshape(-1,ECG_tmp_stim.shape[2])\n",
        "    data_ECG_stim = pd.DataFrame(ECG_tmp_stim, columns=col_stim)\n",
        "\n",
        "    ECG_tmp_basl = ECG_tmp_basl.reshape(-1,ECG_tmp_basl.shape[2])\n",
        "    data_ECG_basl = pd.DataFrame(ECG_tmp_basl, columns=col_basl)\n",
        "\n",
        "    return data_ECG_stim,data_ECG_basl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zphgOAH4DDnL"
      },
      "source": [
        "Ce code utilise la bibliothèque Plotly pour tracer un graphique de signaux ECG pour un participant donné. Les signaux d'ECG sont extraits des données de stimulus et de ligne de base pour les deux canaux RA et LA . Les signaux sont tracés en fonction du temps avec une couleur différente pour chaque signal. Le code définit également un titre pour le graphique ainsi que des étiquettes pour les axes x et y. Le graphique est créé en utilisant les données tracées et la mise en page définie, puis est affiché à l'aide de la fonction \"fig.show()\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He7N00wmkIxp"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "import scipy.signal as signal\n",
        "\n",
        "df_subset_stim_RA = ecg_stim_data.iloc[2,:15616]\n",
        "df_subset_stim_LA = ecg_stim_data.iloc[2,15617:]\n",
        "df_subset_basl_RA = ecg_basl_data.iloc[2, :15616]\n",
        "df_subset_basl_LA = ecg_basl_data.iloc[2, 15617:]\n",
        "\n",
        "trace1 = go.Scatter(x=df_subset_stim_RA.index, y=df_subset_stim_RA.values, mode='lines', name='Stimulus RA', line=dict(color='blue'))\n",
        "trace2 = go.Scatter(x=df_subset_stim_LA.index, y=df_subset_stim_LA.values, mode='lines', name='Stimulus LA', line=dict(color='green'))\n",
        "trace3 = go.Scatter(x=df_subset_basl_RA.index, y=df_subset_basl_RA.values, mode='lines', name='Baseline RA', line=dict(color='red'))\n",
        "trace4 = go.Scatter(x=df_subset_basl_LA.index, y=df_subset_basl_LA.values, mode='lines', name='Baseline LA', line=dict(color='purple'))\n",
        "\n",
        "peaks_stim_RA, _ = signal.find_peaks(df_subset_stim_RA.values, distance=150)\n",
        "trace_peaks_stim_RA = go.Scatter(x=df_subset_stim_RA.index[peaks_stim_RA], y=df_subset_stim_RA.values[peaks_stim_RA], mode='markers', \n",
        "                         name='Pics Stim RA', marker=dict(color='pink', symbol='triangle-down', size=7, line=dict(width=1, color='pink')))\n",
        "\n",
        "peaks_stim_LA, _ = signal.find_peaks(df_subset_stim_LA.values, distance=150)\n",
        "trace_peaks_stim_LA = go.Scatter(x=df_subset_stim_LA.index[peaks_stim_LA], y=df_subset_stim_LA.values[peaks_stim_LA], mode='markers', \n",
        "                         name='Pics Stim LA', marker=dict(color='orange', symbol='triangle-down', size=7, line=dict(width=1, color='orange')))\n",
        "\n",
        "peaks_basl_RA, _ = signal.find_peaks(df_subset_basl_RA.values, distance=150)\n",
        "trace_peaks_basl_RA = go.Scatter(x=df_subset_basl_RA.index[peaks_basl_RA], y=df_subset_basl_RA.values[peaks_basl_RA], mode='markers', \n",
        "                         name='Pics Basl RA', marker=dict(color='green', symbol='triangle-down', size=7, line=dict(width=1, color='green')))\n",
        "\n",
        "peaks_basl_LA, _ = signal.find_peaks(df_subset_basl_LA.values, distance=150)\n",
        "trace_peaks_basl_LA = go.Scatter(x=df_subset_basl_LA.index[peaks_basl_LA], y=df_subset_basl_LA.values[peaks_basl_LA], mode='markers', \n",
        "                         name='Pics Basl LA', marker=dict(color='purple', symbol='triangle-down', size=7, line=dict(width=1, color='purple')))\n",
        "\n",
        "layout = go.Layout(title='Figure des signaux ECG', xaxis=dict(title='temps'), yaxis=dict(title='Signal'))\n",
        "\n",
        "fig = go.Figure(data=[trace1, trace2, trace3, trace4, trace_peaks_stim_RA, trace_peaks_stim_LA, trace_peaks_basl_RA, trace_peaks_basl_LA], layout=layout)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL7P_3d9usup"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "import scipy.signal as signal\n",
        "\n",
        "df_subset_stim_RA = ecg_stim_data.iloc[2,:15616]\n",
        "df_subset_stim_LA = ecg_stim_data.iloc[2,15617:]\n",
        "\n",
        "trace1 = go.Scatter(x=df_subset_stim_RA.index, y=df_subset_stim_RA.values, mode='lines', name='Stimulus RA', line=dict(color='blue'))\n",
        "trace2 = go.Scatter(x=df_subset_stim_LA.index, y=df_subset_stim_LA.values, mode='lines', name='Stimulus LA', line=dict(color='blue'))\n",
        "\n",
        "\n",
        "layout = go.Layout(title='Figure des signaux ECG', xaxis=dict(title='temps'), yaxis=dict(title='Signal'))\n",
        "\n",
        "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faGqY5MczyPY"
      },
      "outputs": [],
      "source": [
        "df_participant_affective = participant_affective(raw)\n",
        "\n",
        "df_participant_affective[\"valence\"] = (df_participant_affective\n",
        "                                       [\"valence\"].astype(int))\n",
        "df_participant_affective[\"arousal\"] = (df_participant_affective\n",
        "                                       [\"arousal\"].astype(int))\n",
        "df_participant_affective[\"dominance\"] = (df_participant_affective\n",
        "                                         [\"dominance\"].astype(int))\n",
        "\n",
        "df = pd.concat([ecg_stim_data, df_participant_affective], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsl52g4T0Vxz"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSZSIJkBDUc7"
      },
      "source": [
        "Ce code crée un nouveau DataFrame ecg_data à partir du DataFrame df contenant les données d'émotion et d'ECG. Le nouveau DataFrame contient uniquement les données pour les émotions sélectionnées ('anger', 'fear', 'calmness', 'surprise', 'excitement', 'amusement', 'happiness', 'sadness' et 'disgust').\n",
        "\n",
        "Ensuite, le code ajoute deux nouvelles colonnes class et valencehigh à ecg_data. La colonne class affecte un entier à chaque émotion, où 0 représente les émotions positives (surprise, excitement, amusement, happiness), 1 représente les émotions négatives (fear, anger, sadness, disgust), et 2 représente la neutralité (calmness).\n",
        "\n",
        "La colonne valencehigh affecte un 1 ou un 0 en fonction de la valence de l'émotion. Les valeurs élevées de valence sont affectées à 1 et les valeurs faibles de valence sont affectées à 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t5MWkNz6M1y"
      },
      "outputs": [],
      "source": [
        "ecg_data = df.loc[(df['target_emotion'] == 'anger') |\n",
        "                (df['target_emotion'] == 'fear') |\n",
        "                (df['target_emotion'] == 'calmness') |\n",
        "                (df['target_emotion'] == 'surprise') |\n",
        "                (df['target_emotion'] == 'excitement') |\n",
        "                (df['target_emotion'] == 'amusement') |\n",
        "                (df['target_emotion'] == 'happiness') |\n",
        "                (df['target_emotion'] == 'sadness') |\n",
        "                (df['target_emotion'] == 'disgust')].copy()\n",
        "\n",
        "d={'surprise': 0, 'excitement': 0, 'amusement': 0, 'happiness': 0, 'fear': 1, 'anger': 1, 'sadness':2, 'disgust':2 ,'calmness': 3}\n",
        "ecg_data['class'] = ecg_data.target_emotion.map(d)\n",
        "\n",
        "e={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "ecg_data['valencehigh'] = ecg_data.valence.map(e)\n",
        "\n",
        "f={0: 0, 1: 0, 2: 0, 3: 1, 4: 1, 5: 1 }\n",
        "ecg_data['arousalhigh'] = ecg_data.arousal.map(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8nDk6Ft3KcS"
      },
      "outputs": [],
      "source": [
        "target = ecg_data['valencehigh']\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U1Vwxdq4hhi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "ecg_stim_data = np.array(ecg_stim_data, dtype = float)\n",
        "target = np.array(target, dtype = float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9z0uibr4r-X"
      },
      "outputs": [],
      "source": [
        "x_Train, x_Test, y_Train, y_Test = train_test_split(ecg_stim_data, target, test_size = 0.2, random_state = 6)\n",
        "\n",
        "x_Train = x_Train.reshape(x_Train.shape[0],1,31232)\n",
        "x_Test = x_Test.reshape(x_Test.shape[0],1,31232)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M353eyLKDdwX"
      },
      "source": [
        "Ce code définit un modèle de réseau de neurones récurrents (RNN) pour la classification de séquences. Le modèle utilise deux couches LSTM (Long Short-Term Memory) avec une fonction d'activation ReLU et une couche dense avec une fonction d'activation sigmoïde. Le modèle est compilé avec l'optimiseur Adam et une fonction de perte de \"sparse_categorical_crossentropy\". Le modèle est ensuite entraîné avec les données d'entraînement et validé avec les données de test. Le nombre d'époques est fixé à 200 et la taille de lot est de 16. Les prédictions du modèle pour les données de test sont stockées dans la variable \"y_pred\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHhRdAkN5bQB"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128,activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64,activation ='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,activation ='relu'))\n",
        "model.add(Dense(2,activation ='sigmoid'))\n",
        "y_pred = model.predict(x_Test)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt,metrics=['accuracy'])\n",
        "history = model.fit(x_Train,y_Train,batch_size=16,epochs=200, validation_data =(x_Test,y_Test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUT8X7OmATwd"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly1ywMLsAWsG"
      },
      "outputs": [],
      "source": [
        " plt.plot(history.history['loss'])\n",
        " plt.title(\"Loss for LSTM model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Loss\")\n",
        " plt.show()\n",
        " plt.plot(history.history['accuracy'])\n",
        " plt.title(\"Accuracy for LSTM model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        " plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE8o6XR8DyMF"
      },
      "source": [
        "Ce code crée un modèle de réseau de neurones à convolution (CNN) pour une classification multiclasse. Le modèle est créé à l'aide de la classe Sequential de Keras, qui permet de créer un modèle linéaire de couches. Le modèle comprend une couche d'aplatissement (flatten) qui transforme l'entrée en un vecteur 1D, deux couches denses (fully connected) avec une fonction d'activation relu, une couche de dropout pour éviter le surapprentissage et une couche dense de sortie avec une fonction d'activation softmax pour prédire les probabilités pour chaque classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpzRP0tLAeR9"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(Dense(128,activation =tf.nn.relu))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation =tf.nn.relu))\n",
        "model.add(Dense(5,activation =tf.nn.softmax))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt ,metrics=['accuracy'])\n",
        "history_CNN = model.fit(x_Train,y_Train,epochs=200, validation_data =(x_Test,y_Test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTnKu1FzBo4P"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I47-2IVBvY3"
      },
      "outputs": [],
      "source": [
        " plt.plot(history_CNN.history['loss'])\n",
        " plt.title(\"Loss for LSTM model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Loss\")\n",
        " plt.show()\n",
        " plt.plot(history_CNN.history['accuracy'])\n",
        " plt.title(\"Accuracy for LSTM model - Valence\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        " plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoKAVQ9LEN1m"
      },
      "outputs": [],
      "source": [
        "target=ecg_data['arousalhigh']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNmRk320Ei7s"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "ecg_stim_data = np.array(ecg_stim_data, dtype = float)\n",
        "target = np.array(target, dtype = float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD5gVWKcEn24"
      },
      "outputs": [],
      "source": [
        "x_Train, x_Test, y_Train, y_Test = train_test_split(ecg_stim_data, target, test_size = 0.2, random_state = 6)\n",
        "\n",
        "x_Train = x_Train.reshape(x_Train.shape[0],1,31232)\n",
        "x_Test = x_Test.reshape(x_Test.shape[0],1,31232)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acsR1FV9I_mg"
      },
      "source": [
        "Fonction de rappel (callback) qui arrête l'entraînement une fois que l'exactitude (accuracy) atteint 80% et que l'exactitude de validation atteint 65%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxJ4-1I5I1sM"
      },
      "outputs": [],
      "source": [
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if logs.get('accuracy') > 0.8 and logs.get('val_accuracy') > 0.65:\n",
        "            self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Z8_tSSJi0_"
      },
      "source": [
        "Dans ce code, un modèle de réseaux de neurones LSTM est créé en utilisant la bibliothèque Keras. Le modèle est composé de deux couches LSTM, chacune suivie d'une couche de régularisation Dropout pour réduire le surapprentissage. Ensuite, deux couches denses avec les fonctions d'activation relu et sigmoid sont ajoutées. Le modèle est compilé avec la fonction de perte \"sparse_categorical_crossentropy\" et l'optimiseur Adam avec un taux d'apprentissage de 1e-3.\n",
        "\n",
        "Le modèle est entraîné avec le jeu de données d'apprentissage \"x_Train\" et \"y_Train\" pour un maximum de 200 époques, avec une taille de lot de 16. Le jeu de données de validation \"x_Test\" et \"y_Test\" est également fourni pour évaluer les performances du modèle après chaque époque. En outre, un objet CustomCallback() est ajouté en tant que callback pour interrompre l'entraînement une fois que la précision de l'apprentissage dépasse 80% et que la précision de validation dépasse 65%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQVgf5kJErW3"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128,activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64,activation ='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,activation ='relu'))\n",
        "model.add(Dense(2,activation ='sigmoid'))\n",
        "y_pred = model.predict(x_Test)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer =opt,metrics=['accuracy'])\n",
        "history = model.fit(x_Train,y_Train,batch_size=16,epochs=200, validation_data =(x_Test,y_Test),callbacks=[CustomCallback()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZEcwzRGJroZ"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlU2Bu4jJtZl"
      },
      "outputs": [],
      "source": [
        " plt.plot(history.history['loss'])\n",
        " plt.title(\"Loss for LSTM model - Arousal\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Loss\")\n",
        " plt.show()\n",
        " plt.plot(history.history['accuracy'])\n",
        " plt.title(\"Accuracy for LSTM model - Arousal\")\n",
        " plt.xlabel(\"Epochs\")\n",
        " plt.ylabel(\"Accuracy\")\n",
        " plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}